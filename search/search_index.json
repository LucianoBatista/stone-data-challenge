{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Challenge Stone 2022 O objetivo dessa p\u00e1gina \u00e9 servir de documneta\u00e7\u00e3o de todo o processo de an\u00e1lise realizado na base fornecida pela Stone. Aqui ser\u00e1 registrado o Framework de Avalia\u00e7\u00e3o. Para mais informa\u00e7\u00f5es sobre o desafio acesse: LINK . Framework de Avalia\u00e7\u00e3o A minha an\u00e1lise sobre os dados da Stone segue duas grandes etapas . Primeiramente foi realizado uma an\u00e1lise mais descritiva dos dados, com o objetivo de conhecer detalhes dos dados e poss\u00edveis inconsist\u00eancias. E posteriormente, uma an\u00e1lise mais direcionada para resolu\u00e7\u00e3o do problema. Nas valida\u00e7\u00f5es iniciais ser\u00e1 registrado a etapa descritiva, e em fluxo de etapas realizadas ser\u00e1 registrado os passos para chegar na melhor curva de acionamento do cliente. Valida\u00e7\u00f5es iniciais Como dito anteriormente, nesta etapa foi realizada uma an\u00e1lise descritiva das bases que est\u00e3o dispon\u00edveis para trabalhar. Nesse caso n\u00f3s tempos: portfolio_clientes portfolio_comunicados portfolio_geral portfolio_tpv # libs import pandas as pd import numpy as np import matplotlib.pyplot as plt from src.make_plot import plot # matplotlib configs plt . rcParams [ \"figure.figsize\" ] = ( 12 , 7 ) plt . rcParams [ \"figure.dpi\" ] = 200 # data clients = pd . read_csv ( \"data/portfolio_clientes.csv\" ) tpv = pd . read_csv ( \"data/portfolio_tpv.csv\" ) comunicados = pd . read_csv ( \"data/portfolio_comunicados.csv\" ) geral = pd . read_csv ( \"data/portfolio_geral.csv\" ) 1. portfolio_clientes Aqui encontramos todas as informa\u00e7\u00f5es relacionadas aos dados cadastrais dos clientes que contratam empr\u00e9stimos, assim como dados geogr\u00e1ficos e segmentos do neg\u00f3cio. A base deveria possuir como registro \u00fanico, a coluna de nr_documento , por\u00e9m foi visto que existem registros duplicados. Devido a essa inconsist\u00eancia, os contratos relacionadas aos nr_documento duplicados n\u00e3o ser\u00e3o considerados. Vamos ent\u00e3o calcular o total de clientes registrados: # total de clientes clients_wo_duplicate = clients . drop_duplicates ([ \"nr_documento\" ]) clients_wo_duplicate . shape ( 14265 , 6 ) Dessa forma, possu\u00edmos um total de 14.265 clientes na base. Esse valor n\u00e3o reflete diretamente na quantidade de contrato j\u00e1 que cada cliente pode possuir mais de um contrato. Apenas a n\u00edvel j\u00e1 realizarmos um comparativo, puxamos tamb\u00e9m o n\u00famero de contratos registrados na base: # total de clientes geral . drop_duplicates ( \"contrato_id\" ) . shape ( 14756 , 22 ) Trazendo a informa\u00e7\u00e3o da tabela portfolio_geral vemos que existe um n\u00famero levemente maior, 14.756 contratos , registrados. Essa base \u00e9 composta de mais algumas vari\u00e1veis categ\u00f3ricas, e saber como as mesmas est\u00e3o distribu\u00eddas pode nos trazer bons insights para an\u00e1lises futuras mais complexas. Distribui\u00e7\u00e3o de Tipo da Empresa # getthering the data to plot counts = clients . value_counts ( \"tipo_empresa\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"tipo_empresa\" ] . values # this function is part of the code on src directory plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o do Tipo de Empresa\" , \"Tipos de Empresas\" , \"Quantidade de Clientes em cada Empresa\" , False ) A maioria dos clientes est\u00e3o cadastrados como Pessoa Jur\u00eddica , e bem pouco como Pessoal F\u00edsica . Distribui\u00e7\u00e3o dos Estados counts = clients . value_counts ( \"estado\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"estado\" ] . values # this function is part of the code on src directory plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o de quantidade de clientes em cada Estado.\" , \"Estados brasileiros\" , \"Quantidade de clientes em cada Estado\" , False ) Aqui vemos uma grande maioria dos clientes pertecentes ao Estado de S\u00e3o Paulo , seguido pelo Rio de Janeiro . A diferen\u00e7a dentre os outros Estados vai decaindo gradualmente. Distribui\u00e7\u00e3o dos Segmentos e Subsegmentos counts = clients . value_counts ( \"segmento\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"segmento\" ] . values # this function is part of the code on src directory plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o de quantidade de clientes em cada segmento.\" , \"Segmentos\" , \"Quantidade de clientes em cada segmento\" , True ) Quando olhamos para os segmentos, \u00e9 poss\u00edvel ver uma predomin\u00e2ncia do setor de Alimenta\u00e7\u00e3o , seguido de Verejo , Bens Dur\u00e1veis , Servi\u00e7os e Supermercado/Farm\u00e1cia , com uma minoria em outras 4 categorias. counts = clients . value_counts ( \"subsegmento\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"subsegmento\" ] . values # this function is part of the code on src directory plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o de quantidade de clientes em cada subsegmento.\" , \"Subsegmentos\" , \"Quantidade de clientes em cada subsegmento\" , True ) J\u00e1 nos Subsegmentos, temos duas grandes representatividades, Alimenta\u00e7\u00e3o R\u00e1pida e Lojas Diversas. De Outros em diante, a frequ\u00eancia de clientes vai caindo gradualmente. Um ponto interessante a se observar \u00e9 que a maior quantidade de clientes est\u00e1 relacionada a Alimenta\u00e7\u00e3o , seja diretamente, como indiretamente dentro dos top 5, como: Bares e Restaurantes e Supermercados. Interessante a se notar tamb\u00e9m \u00e9 que temos a presen\u00e7a de um subsegmento chamado None como uma string v\u00e1lida, quando na verdade categoriza um missing value e n\u00e3o um subsegmento de fato. 2. portfolio_tpv Essa tabela tr\u00e1s toda a informa\u00e7\u00e3o de Total Paid Value que representa o valor transacionado no dia por cada um dos clientes pela maquininha Stone. Aqui buscou-se tamb\u00e9m pela contagem de quantos nr_documento existem na tabela, chegando no valor de 14.259 clientes . tpv . drop_duplicates ( \"nr_documento\" ) . shape ( 14259 , 4 ) Veja que aqui n\u00f3s temos uma diverg\u00eancia em rela\u00e7\u00e3o \u00e0 quantidade de nr_documento presente na tabela portfolio_geral . Um total de 6 clientes n\u00e3o possuem registro de TPV, por\u00e9m, quando analisados na portfolio_geral vemos que os mesmos possuem valor de pagamento realizado, caracterizados pela presen\u00e7a de m\u00e9dia. nr_documento_tpv = tpv . drop_duplicates ( \"nr_documento\" )[ \"nr_documento\" ] . to_list () geral_for_not_tpv = geral [ ~ geral [ \"nr_documento\" ] . isin ( nr_documento_tpv )] geral_for_not_tpv [[ \"vlr_desembolsado\" , \"vlr_pgto_realizado\" ]] . describe () stats vlr_desembolsado vlr_pgto_realizado count 3938.00 3938.00 mean 11854.92 11.86 std 7246.29 176.45 min 384.75 0.00 25% 3078.00 0.00 50% 17100.00 0.00 75% 17100.00 0.00 max 20520.00 5969.64 Olhando um pouco mais no detalhe, \u00e9 poss\u00edvel ver que alguns dos clientes de fato n\u00e3o tiveram nenhum pagamento realizado, o que justifica o mesmo n\u00e3o possuir registro na tabela portfolio_tpv . geral_for_not_tpv . groupby ([ \"nr_documento\" ])[ \"vlr_pgto_realizado\" ] . agg ([ \"mean\" ]) nr_documento mean 0a6b34a6b108015777d83b1023d43342 23.359794 1eb088b95b56970c880030922dce1c85 17.530547 69116fe5b82f759fd2f295f1daa92ecf 0.000000 6ef839f0201c6295072e45a95eb34466 10.529941 e6addfdeb74a038bb5f7149c7cfb1290 17.557847 ec55907309c0e6195675cb786f7d7242 0.000000 Analisar o TPV como um todo n\u00e3o \u00e9 muito conclusivo, temos valores muito dispersos de valor transacionado diariamente, assim como de m\u00e1ximos e m\u00ednimos. tpv [[ \"qtd_transacoes\" , \"vlr_tpv\" ]] . describe () . apply ( lambda s : s . apply ( \" {0:.5f} \" . format )) stats dt_transacao qtd_transacoes vlr_tpv count 4408597.00 4408597.00 4408597.00 mean 20204274.56 15.91 887.50 std 4656.32 26.15 1664.21 min 20200101.00 -2.00 -125000.00 25% 20200624.00 3.00 175.00 50% 20201030.00 7.00 430.00 75% 20210226.00 18.00 978.00 max 20210630.00 1245.00 176880.93 Portanto, como temos os dados cadastrais dos clientes, \u00e9 mais interessante levar isso em considera\u00e7\u00e3o na hora fazer a an\u00e1lise. Vamos primeiramente juntar as bases: tpv_merged = tpv . merge ( right = clients , on = \"nr_documento\" , how = \"inner\" ) Analisando primeiramente o TPV para segmento e subsegmento, temos: tpv_sum_segment = tpv_merged . groupby ([ \"segmento\" ])[ \"vlr_tpv\" ] . agg ([ \"sum\" ]) . reset_index () tpv_sum_segment [ \"prop\" ] = tpv_sum_segment [ \"sum\" ] / tpv_sum_segment [ \"sum\" ] . sum () tpv_sum_subsegment = tpv_merged . groupby ([ \"subsegmento\" ])[ \"vlr_tpv\" ] . agg ([ \"sum\" ]) . reset_index () tpv_sum_subsegment [ \"prop\" ] = tpv_sum_subsegment [ \"sum\" ] / tpv_sum_subsegment [ \"sum\" ] . sum () tpv_sum_segment . sort_values ( \"prop\" , ascending = False ) segmento sum prop Alimenta\u00e7\u00e3o 1.473060e+09 0.372553 Bens dur\u00e1veis 7.342777e+08 0.185707 Varejo 5.917481e+08 0.149660 Supermercado/Farm\u00e1cia 4.799287e+08 0.121379 Servi\u00e7os 4.409396e+08 0.111518 Posto 8.120651e+07 0.020538 Servi\u00e7os recorrentes 6.621661e+07 0.016747 Viagens e entretenimento 4.964331e+07 0.012555 Outros 3.694166e+07 0.009343 Aqui vemos que a ordem muda um pouco, onde Varejo perde posi\u00e7\u00e3o para Bens Dur\u00e1veis e Servi\u00e7os para Supermercado/Farm\u00e1cia tpv_sum_subsegment . sort_values ( \"prop\" , ascending = False ) subsegmento sum prop Alimenta\u00e7\u00e3o R\u00e1pida 7.782735e+08 0.196834 Lojas Diversas 6.846034e+08 0.173144 Supermercados 4.372940e+08 0.110596 Bares e Restaurantes 3.662225e+08 0.092622 Outros 3.469090e+08 0.087737 Automotivo 2.711954e+08 0.068588 Sa\u00fade 2.537558e+08 0.064178 Material de Constru\u00e7\u00e3o 2.178250e+08 0.055090 Vestu\u00e1rio 2.070685e+08 0.052370 Atacadistas Gerais 1.396476e+08 0.035318 Postos de Gasolina 8.112604e+07 0.020518 Educa\u00e7\u00e3o 4.507371e+07 0.011400 None 4.076986e+07 0.010311 Lazer & Turismo 3.600002e+07 0.009105 Atacadista de Alimento 2.495160e+07 0.006311 Academias 2.324615e+07 0.005879 Em rela\u00e7\u00e3o ao subsegmento, vemos que houve uma troca de posi\u00e7\u00e3o entre Supermercados e Outros . Com essas duas bases podemos buscar por sasonalidade nos dados e melhor compreender como cada neg\u00f3cio oscila ao passar do tempo. Para isso, iremos utilizar uma vis\u00e3o de heatmap variando na horizontal o meses do ano, e na vertical todos os nossos Estados, e o valor mapeado no heatmap \u00e9 o somat\u00f3rio do TPV naquele espec\u00edfico grupo. Vamos criar as vari\u00e1veis temporais que iremos precisar e agroupar os dados que ser\u00e3o utilizados para o heatmap: tpv_merged [ \"dt_transacao\" ] = pd . to_datetime ( tpv_merged [ \"dt_transacao\" ], format = \"%Y%m %d \" ) tpv_merged [ \"month\" ] = tpv_merged [ \"dt_transacao\" ] . dt . month tpv_merged [ \"year\" ] = tpv_merged [ \"dt_transacao\" ] . dt . year # grouping df = ( tpv_merged . groupby ([ \"segmento\" , \"estado\" , \"year\" , \"month\" ])[ \"vlr_tpv\" ] . agg ([ \"sum\" ]) . reset_index () ) # labels for the plot segmentos = list ( df [ \"segmento\" ] . unique ()) monhts = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ] # this code will generate all heatmaps for segmento in segmentos : df_segmento = df [( df [ \"year\" ] == 2020 ) & ( df [ \"segmento\" ] == segmento )] estados = list ( df_segmento [ \"estado\" ] . unique ()) df_heatmap = df_segmento . drop ([ \"segmento\" ], axis = 1 ) . pivot_table ( values = \"sum\" , index = [ \"estado\" ], columns = [ \"month\" ] ) fig , ax = plt . subplots () im , cbar = plot . heatmap ( df_heatmap , estados , monhts , ax = ax , cmap = \"YlGn\" , cbarlabel = \"Total TPV\" ) # Number of accent colors in the color scheme plt . title ( segmento ) fig . tight_layout () plt . show () Como temos diferentes tipos de segmentos, achei mais prudente trazer apenas os Top 3 segmentos de maior TPV para interpretarmos os heatmaps. O que vemos aqui \u00e9 que o valor do total de TPV transacionado ao longo do ano de 2020 foram bem semelhantes, com predom\u00ednio de S\u00e3o Paulo e Rio de Janeiro (por serem os Estados com mais clientes) e a predomin\u00e2ncia tamb\u00e9m do per\u00edodo do segundo trimestre ter sido um per\u00edodo de baixa e que provavelmente deve ter impactado o valor pago nas maquininhas. O Subsegmento tamb\u00e9m demonstrou comportamento semelhante ao considerar os Top 3 pelo valor do TPV. A \u00fanica diferen\u00e7a \u00e9 que acabamos tendo uma variabilidade maior entre os Estados e ao longo do ano para subsegmentos menores, como Academia e Atacadista de Alimento . Acredito que tal comportamento deveria ser levado em considera\u00e7\u00e3o ao estabelecer uma r\u00e9gua de acionamento. Pois at\u00e9 mesmo, as cidades dentro de Estados maiores, como S\u00e3o Paulo e Rio de Janeiro tamb\u00e9m podem apresentar realidades diferentes por Segmentos e Subsegmentos. 3. portfolio_comunicados Essa tabela possui todo o dado de quem foi acionado no seu hist\u00f3rico de empr\u00e9stimo de linha de cr\u00e9dito. Ao longo desse per\u00edodo, 403.704 acionamentos foram realizados, onde 47.35% sofreram algum problema e acabaram por n\u00e3o serem entregues, 34% foram de fato entregues, 17.6 foram lidas e 0.08 foram respondidas. comunicados . value_counts ( \"status\" , normalize = True ) Tamb\u00e9m observou-se que os acionamentos s\u00e3o realizados por dois canais, mensagens diretas e email . Por\u00e9m, ao analisar os subtotais dentro de cada status, vemos que n\u00e3o existe prefer\u00eancia entre os tipos canais utilizados comunicados . value_counts ([ \"status\" , \"tipo_acao\" ], normalize = True ) . reset_index () status tipo_acao prop NAO ENTREGUE HSM 0.236836 NAO ENTREGUE EMAIL 0.236754 ENTREGUE HSM 0.171046 ENTREGUE EMAIL 0.170637 LIDO HSM 0.088706 LIDO EMAIL 0.088091 RESPONDIDO EMAIL 0.004518 RESPONDIDO HSM 0.003413 Todos esses acionamentos foram realizados para um total de 12.202 contratos , o que representa aproximadamente 83% da base . Um n\u00famero bastante alto de clientes que apresentaram algum tipo de inadipl\u00eancia. E por fim, pelo gr\u00e1fico abaixo, podemos ver que 52% das campanhas realizadas foram de observa\u00e7\u00e3o , seguido de parcelamento e boleto quitado. counts = comunicados . value_counts ( \"acao\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"acao\" ] . values plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o do Tipo de A\u00e7\u00e3o\" , \"Tipos de A\u00e7\u00f5es\" , \"Quantidade de A\u00e7\u00f5es realizadas\" , False ) 4. portfolio_geral Essa \u00e9 a maior tabela que temos e a mesma tem todo o hist\u00f3rico de cada um dos mais de 12k de contratos registrados. Com essa tabela farei algumas sumariza\u00e7\u00f5es e cria\u00e7\u00e3o de m\u00e9tricas envolvendo o dsp (Dias Corridos sem Pagamento) e o dspp (Dias Corridos sem Pagamento do Principal), que ser\u00e3o descritas na pr\u00f3xima do framework de avalia\u00e7\u00e3o. Essa tabela, em conjunto com dados de todas as outras, ir\u00e1 nos auxiliar a responder o nosso problema principal: Qual \u00e9 a quantidade ideal de vezes que acionamos um cliente? . Fluxo de etapas realizadas Tudo que foi feito at\u00e9 o momento pode ser encontrado no notebook describe.ipynb . Daqui pra frente iremos consumir dos notebooks final_composition.ipynb e analysis.ipynb . Onde o primeiro \u00e9 o respons\u00e1vel por juntar as tabelas e o segundo possui o desenrolar da an\u00e1lise. Como output, o notebook final_composition.ipynb ir\u00e1 nos fornecer um dataset que permitir\u00e1 uma an\u00e1lise mais robusta e a entender a melhor curva de acionamento ao cliente. 1. Merge e Cria\u00e7\u00e3o de Features Primeiramente, vamos precisar trazer todos as nossas tabelas e as libs que ser\u00e3o utilizadas. import pandas as pd from src.feature_engineering import features import numpy as np clients = pd . read_csv ( \"data/portfolio_clientes.csv\" ) tpv = pd . read_csv ( \"data/portfolio_tpv.csv\" ) comunicados = pd . read_csv ( \"data/portfolio_comunicados.csv\" ) geral = pd . read_csv ( \"data/portfolio_geral.csv\" ) Aqui vamos utilizar um m\u00f3dulo de feature engineering que criei para nos auxiliar na cria\u00e7\u00e3o das features de uma forma mais modular e clean. Detalhes sobre essas etapas espec\u00edficas de feature engineering v\u00e3o ser retratadas em uma p\u00e1gina b\u00f4nus da documenta\u00e7\u00e3o. Geral + Comunicados Como visto anteriormente, nem todos os clientes receberam acionamento, por isso, vamos precisar filtrar a base para contemplar apenas os casos que possuem acionamento. # contratos \u00fanicos na tabela de comunicados unique_contratos = comunicados [ \"contrato_id\" ] . unique () # filtrando por esses contratos geral_comunicados = geral [ geral [ \"contrato_id\" ] . isin ( unique_contratos )] # como cada contrato possui mais de um acionamento, para evitar duplicidades # vamos agregar em uma lista os duplicados. comunicados_grouped = ( comunicados . groupby ([ \"contrato_id\" , \"dt_ref_portfolio\" , \"data_acao\" ])[ [ \"tipo_acao\" , \"status\" , \"acao\" ] ] . agg ( list ) . reset_index () ) geral_comunicados_grouped = geral_comunicados . merge ( right = comunicados_grouped , how = \"left\" , on = [ \"contrato_id\" , \"dt_ref_portfolio\" ] ) Veja que estamos utilizando um left-join aqui por que a tabela portfolio_geral possui valores di\u00e1rios, e nem todos os dias ocorreram acionamentos, por\u00e9m, ter essa informa\u00e7\u00e3o \u00e9 muiti \u00fatil para as pr\u00f3ximas etapas. Por fim, vamos realizar um sort nos dados, para garantir que o dsp e o dspp estar\u00e3o na ordem correta mais pra frente, quando formos criar as novas features. geral_and_comunicados_sorted_df = geral_comunicados_grouped . sort_values ( [ \"contrato_id\" , \"dt_ref_portfolio\" ] ) Cria\u00e7\u00e3o de features de DSP e DSPP Aqui est\u00e1 uma etapa muito importante do fluxo da an\u00e1lise dos dados. Uma informa\u00e7\u00e3o que precisamos ter para prosseguir com a an\u00e1lise \u00e9 se os acionamentos foram efetivos para retornar o cliente para a utiliza\u00e7\u00e3o da maquininha da Stone. Tal feature aqui vai ser entendidad da seguinte forma: Se o cliente retornou sua utiliza\u00e7\u00e3o da maquininha dentro de per\u00edodo que antecede o acionamento subsequente, configura SUCESSO. Com isso em mente, criou-se o m\u00f3dulo de feature_engineering , onde parte das fun\u00e7\u00f5es contemplam exatamente esse c\u00e1lculo. Os dois blocos de c\u00f3digo abaixo apresentam como as fun\u00e7\u00f5es s\u00e3o aplicadas por interm\u00e9dio do pandas . # para o dsp contrato_dsp_features = ( geral_and_comunicados_sorted_df . groupby ([ \"contrato_id\" ])[ \"dsp\" ] . agg ( [ features . total_success_dsp5 , features . total_success_dsp10 , features . total_success_dsp15 , features . total_success_dsp30 , features . total_success_dsp60 , features . total_success_dsp90 , features . prop_success_dsp5 , features . prop_success_dsp10 , features . prop_success_dsp15 , features . prop_success_dsp30 , features . prop_success_dsp60 , features . prop_success_dsp90 , ] ) . reset_index () ) # para o dspp contrato_dspp_features = ( geral_and_comunicados_sorted_df . groupby ([ \"contrato_id\" ])[ \"dspp\" ] . agg ( [ features . total_success_dspp15 , features . total_success_dspp30 , features . total_success_dspp45 , features . prop_success_dspp15 , features . prop_success_dspp30 , features . prop_success_dspp45 , ] ) . reset_index () ) # merging os dois sets de features criadas contrato_dsp_dspp = contrato_dsp_features . merge ( right = contrato_dspp_features , on = \"contrato_id\" , how = \"inner\" ) Veja que, aqui temos n\u00e3o s\u00f3 o total de sucesso em cada um dos acionamentos, mas tamb\u00e9m uma propor\u00e7\u00e3o entre: acionamentos_com_sucesso / total_de_acionamentos Por fim, para juntar esses valores num score mais representativo , vamos calcular as m\u00e9dias das propor\u00e7\u00f5es, considerando os casos onde de fato o cliente recebeu acionamento. means_dsp = [] means_dspp = [] for i , row in contrato_dsp_dspp . iterrows (): means_dsp . append ( np . nanmean ( row [ 7 : 13 ])) means_dspp . append ( np . nanmean ( row [ 16 : 19 ])) contrato_dsp_dspp [ \"score_dsp\" ] = means_dsp contrato_dsp_dspp [ \"score_dspp\" ] = means_dspp Entregou? N\u00e3o Entregou? Leu? Um outro ponto muito importante que devemos levar em considera\u00e7\u00e3o \u00e9 se o cliente de fato leu a mensagem ou se o mesmo at\u00e9 mesmo recebeu, j\u00e1 que vimos que o n\u00famero de clientes que n\u00e3o recebem o acionamento (quando deveria ter recebido) \u00e9 bastante alto, cerca de 47%. acionamentos_delivery = ( geral_and_comunicados_sorted_df . groupby ([ \"contrato_id\" ])[ \"status\" ] . agg ([ features . get_entregue , features . get_lido , features . get_nao_entregue ]) . reset_index () ) # merge para adicionar ao nosso dataset final as features de entregue, n\u00e3o entregue e lido contrato_dsp_dspp_qtd_acoes = contrato_dsp_dspp . merge ( right = acionamentos_delivery , how = \"inner\" , on = \"contrato_id\" ) Aqui, tamb\u00e9m estamos utilizando o m\u00f3dulo de feature_engineering . Valor devedor esperado Essa feature pode nos ajudar a entender se existe alguma rela\u00e7\u00e3o entre o valor total de empr\u00e9stimo do cr\u00e9dito com alguma outra feature que iremos trazer para compor a an\u00e1lise final. Como essa informa\u00e7\u00e3o n\u00e3o estava expl\u00edcita, resolvi considerar o valor devedor esperado no primeiro dia do contrato como valor de empr\u00e9stimo daquele cliente. # features de vlr_saldo_devedor vlr_saldo_devedor_inicial = geral_and_comunicados_sorted_df . drop_duplicates ( [ \"contrato_id\" ] )[[ \"contrato_id\" , \"vlr_saldo_devedor_esperado\" ]] c_dsp_dspp_qtd_acoes_devedor = contrato_dsp_dspp_qtd_acoes . merge ( right = vlr_saldo_devedor_inicial , how = \"inner\" , on = \"contrato_id\" ) Trazendo dados cadastrais Nossa tabela at\u00e9 o momento possui o contrato_id e algumas features que coletamos. Por\u00e9m, para cruzar com os dados cadastrais dos clientes, vamos precisar tamb\u00e9m do nr_documento . Para isso criamos uma tabela intermedi\u00e1ria que vai nos ajudar a trazer os dados dos clientes para essa base. # tabela intermedi\u00e1ria x_contrato_id_nr_documento = geral_and_comunicados_sorted_df . drop_duplicates ( [ \"contrato_id\" , \"nr_documento\" ] )[[ \"contrato_id\" , \"nr_documento\" ]] # trazendo os nr_documentos c_dsp_dspp_qtd_acoes_devedor_w_doc = c_dsp_dspp_qtd_acoes_devedor . merge ( right = x_contrato_id_nr_documento , how = \"inner\" , on = \"contrato_id\" ) # alguns nr_documentos est\u00e3o duplicados por que o cliente # pode ter mais de uma loja ou em diferentes regi\u00f5es # por isso estou agrupando os casos onde acontece duplicatas clientes_unique_nr_doc = ( clientes . groupby ( \"nr_documento\" )[ [ \"tipo_empresa\" , \"cidade\" , \"estado\" , \"subsegmento\" , \"segmento\" ] ] . agg ( lambda x : list ( x ) if len ( x ) > 1 else x ) . reset_index () ) # etapa final, de fato trazendo os dados dos clientes pra base c_dsp_dspp_qtd_acoes_devedor_w_doc_and_clients = c_dsp_dspp_qtd_acoes_devedor_w_doc . merge ( right = clientes_unique_nr_doc , on = \"nr_documento\" , how = \"inner\" ) + TPV Agora, vamos trazer a informa\u00e7\u00e3o do TPV, sumerizado por cliente. Aqui teremos: m\u00ednimo m\u00e1ximo m\u00e9dia mediana soma Para quantidade de transa\u00e7\u00e3o realizada no dia e para o valor do tpv. qtd_trans_tpv = tpv . groupby ( \"nr_documento\" )[[ \"qtd_transacoes\" , \"vlr_tpv\" ]] . agg ( [ \"mean\" , \"min\" , \"max\" , np . median , \"sum\" ] ) final_df = c_dsp_dspp_qtd_acoes_devedor_w_doc_and_clients . merge ( right = qtd_trans_tpv , how = \"left\" , on = \"nr_documento\" ) # saving our dataset final_df . to_csv ( \"data/to_analysis.csv\" , index = False ) 2. An\u00e1lise Explorat\u00f3ria (Dataset final) Agora, com nosso novo dataset, n\u00f3s temos cerca de 41 features as quais podemos utilizar para chegar a conclus\u00e3o de qual a melhor curva de acionamento do cliente. Por quest\u00e3o de tempo e disponibilidade, irei focar em duas an\u00e1lises na tentativa de encontrar algum padr\u00e3o que nos direcione para nosso objetivo. A primeira ser\u00e1 uma an\u00e1lise do score_dsp e score_dspp com features de segmento e subsegmento, e em segundo, do score_dsp e score_dspp com features que indicam se as mensagens foram de fato entregues, lidas ou se simplesmente n\u00e3o foram entregues. Vale pontuar aqui que esses scores foram calculado, tirando a m\u00e9dia da porcentagem de sucesso ao aplicar uma campanha. E tamb\u00e9m que poderia muito bom cada uma dessas campanhas possuirem um peso espec\u00edfico, por\u00e9m nesse primeiro momento vamos considerar todas as campanhas com o mesmo peso. DSP e DSPP pelo Segmento e Subsegmento Vamos iniciar filtrando o dados para retornar apenas os dados que vamos utilizar. # intially by the dsp df_filtered = df [[ \"nr_documento\" , \"score_dsp\" , \"segmento\" ]][ ( df [ \"segmento\" ] . isin ( segmentos )) & ( df [ \"subsegmento\" ] . isin ( subsegmentos )) ] Aqui tamb\u00e9m estamos filtrando pelas categorias nos segmentos e subsegmentos, removendo os casos de nr_documento duplicado. Vamos ent\u00e3o analisar um boxplot para cada uma das categorias e ter uma vis\u00e3o de como est\u00e3o distribu\u00eddos os scores. fig = px . box ( df_filtered , x = \"segmento\" , y = \"score_dsp\" ) fig . show () Com esse gr\u00e1fico, podemos ver que existe uma vari\u00e2ncia alta nos dados, e independemente do segmento, existem valores que v\u00e3o do 0 at\u00e9 o 1. Por\u00e9m, ao olharmos para a mediana, vemos que Servi\u00e7os recorrentes possuem os valores mais altos desse do score_dsp . Agora, para o subsegmento: # intially by the dsp df_filtered = df [[ \"nr_documento\" , \"score_dsp\" , \"subsegmento\" ]][ ( df [ \"segmento\" ] . isin ( segmentos )) & ( df [ \"subsegmento\" ] . isin ( subsegmentos )) ] fig = px . box ( df_filtered , x = \"subsegmento\" , y = \"score_dsp\" ) fig . show () O mesmo comportamento do anteior pode ser visto aqui. Os valores de mediana mais altos est\u00e3o por conta do subsegmento Educa\u00e7\u00e3o . E setores relacionados a Alimenta\u00e7\u00e3o, direta ou indiretamente, possuem valores em torno de 0.5 de mediana no score_dsp . Vamos visualizar o mesmo para o dspp: df_filtered = df [[ \"nr_documento\" , \"score_dspp\" , \"segmento\" , \"subsegmento\" ]][ ( df [ \"segmento\" ] . isin ( segmentos )) & ( df [ \"subsegmento\" ] . isin ( subsegmentos )) ] fig = px . box ( df_filtered , x = \"segmento\" , y = \"score_dspp\" ) fig . show () fig = px . box ( df_filtered , x = \"subsegmento\" , y = \"score_dspp\" ) fig . show () Pelo que vemos aqui, tanto para segmento, quanto para subsegmento, apesar de termos valores oscilando de 0 a 1 em todas as categorias, as medianas aqui s\u00e3o bem menores que no dsp. Podemos inferir aqui que o score n consegue ser explicado apenas pelo segmento e subsegmento, sendo necess\u00e1rio mais vari\u00e1veis, vamos seguir para a utiliza\u00e7\u00e3o das features de entrega do acionamento. DSP e DSPP pela Entrega do Acionamento Estou considerando aqui a entrega do acionamento pelas features de: ENTREGUE N\u00c3O ENTREGUE LIDO E, constru\u00edndo os boxplots novamente, temos: df_filtered = df [ [ \"nr_documento\" , \"score_dsp\" , \"get_entregue\" , \"get_nao_entregue\" , \"get_lido\" ] ][( df [ \"segmento\" ] . isin ( segmentos )) & ( df [ \"subsegmento\" ] . isin ( subsegmentos ))] # melting the data df_filtered_melted = df_filtered . melt ( id_vars = [ \"nr_documento\" , \"score_dsp\" ], value_vars = [ \"get_entregue\" , \"get_nao_entregue\" , \"get_lido\" ], var_name = \"acionamento\" , ) fig = px . box ( df_filtered_melted , x = \"acionamento\" , y = \"score_dsp\" ) fig . show () Curiosamente, apesar de nada muito \u00fatil, n\u00e3o conseguimos ver nenhuma rela\u00e7\u00e3o entre as vari\u00e1veis de acionamento e os scores para o dsp. O mesmo foi feito para o dspp, mas obtive o mesmo resultado, n\u00e3o demonstrando nada muito informativo, por isso o gr\u00e1fico n\u00e3o foi plotado aqui. 3. Dashboarding Ap\u00f3s realizada a an\u00e1lise acima, cheguei a conclus\u00e3o que a identifica\u00e7\u00e3o das melhores features que podem nos ajudar a explicar a melhor curva de acionamento do cliente \u00e9 um trabalho que existe mais conhecimento de neg\u00f3cio e mais tentativa e erro. Para otimizar essa an\u00e1lise, irei apresentar um dashboard com alguns filtros e visualiza\u00e7\u00f5es sobre a mediana do sucesso em cada uma das campanhas, por cliente. Dessa forma conseguimos empoderar os usu\u00e1rios com dados e otimizar a tomada de decis\u00e3o. O que teremos de c\u00f3digo dentro do dash ser\u00e1 como o exibido abaixo: # sele\u00e7\u00e3o de filtros filtro_estado = \"SP\" filtro_cidade = \"S\u00e3o Paulo\" prop_columns = [ column for column in df . columns if column . startswith ( \"prop_\" )] # aplica\u00e7\u00e3o dos filtros df_filtered = df [( df [ \"estado\" ] == filtro_estado ) & ( df [ \"cidade\" ] == filtro_cidade )][ prop_columns ] df_melted = df_filtered . melt ( value_vars = prop_columns , var_name = \"props\" ) to_plot = df_melted . groupby ([ \"props\" ])[ \"value\" ] . agg ( np . nanmedian ) . reset_index () # transformando no tipo Categories, para ordenar o plot prop_categories = CategoricalDtype ( [ \"prop_success_dsp5\" , \"prop_success_dsp10\" , \"prop_success_dsp15\" , \"prop_success_dsp30\" , \"prop_success_dsp60\" , \"prop_success_dsp90\" , \"prop_success_dspp15\" , \"prop_success_dspp30\" , \"prop_success_dspp45\" , ], ordered = True ) to_plot [ \"props\" ] = to_plot [ \"props\" ] . astype ( prop_categories ) to_plot_sorted = to_plot . sort_values ( \"props\" ) # plot fig = px . bar ( to_plot_sorted , x = \"props\" , y = \"value\" ) fig . show () A interpreta\u00e7\u00e3o \u00e9 que, para esse conjunto de clientes, filtrados para S\u00e3o Paulo e tamb\u00e9m cidade de S\u00e3o Paulo, tivemos uma convers\u00e3o alta para o dsp5, e a mesma foi declinando at\u00e9 o dsp30. J\u00e1 para o DSPP, tivemos uma convers\u00e3o bem baixa, para os casos onde houve esse tipo de acionamento, chegando a 20%. Conclus\u00f5es e Insights Optei nesse projeto por uma abordagem mais simples, que consistiu uma an\u00e1lise descritiva seguido de uma sumeriza\u00e7\u00e3o dos dados, levando em considera\u00e7\u00e3o cada um dos contratos e clientes presentes na base. O objetivo foi de conseguir insights iniciais que possam nortear melhor a tomada de decis\u00e3o sobre qual a melhor curva de acionamento do cliente. Nesse quesito, cheguei em um score de sucesso de cada um dos acionamentos direcionados ao cliente. Por\u00e9m, esse score sozinho n\u00e3o \u00e9 suficiente para obtermos a melhor curva de acionamento, e o mesmo precisa ser cruzado com outras features ( segunda etapa de an\u00e1lise do projeto ). Ao final dessa segunda etapa, nosso dataset possuia 41 features, e isso foi inevit\u00e1vel dada a complexidade e nuances presentes no dados. Devido a isso, acredito que simples an\u00e1lises descritivas esbarram na limita\u00e7\u00e3o de quantas vari\u00e1veis conseguimos visualizar e buscar por padr\u00f5es. O que deixa claro que esse \u00e9 um problema que pode ser facilmente abordado como um problema de recomenda\u00e7\u00e3o, com base em todas as features criadas nesse processo de an\u00e1lise de dados. Outro ponto importante \u00e9 trazer para esse problema o conceito de experimenta\u00e7\u00e3o , para possibilitar a utiliza\u00e7\u00e3o de modelos de Uplift , e assim saber de fato qual grupo de clientes devem ser abordados com os acionamentos. Infelizmente n\u00e3o consegui chegar a uma curva ideal, mas pude compreender que esse problema que pode ser abordado levando em considera\u00e7\u00e3o a realidade de cada cliente presente na base, deixando o atendimento mais personalisado. Gostaria de finalizar agradecendo a oportunidade fornecida pela Stone em estar participando e aprendendo muito com esse desafio.","title":"Framework de Avalia\u00e7\u00e3o"},{"location":"#data-challenge-stone-2022","text":"O objetivo dessa p\u00e1gina \u00e9 servir de documneta\u00e7\u00e3o de todo o processo de an\u00e1lise realizado na base fornecida pela Stone. Aqui ser\u00e1 registrado o Framework de Avalia\u00e7\u00e3o. Para mais informa\u00e7\u00f5es sobre o desafio acesse: LINK .","title":"Data Challenge Stone 2022"},{"location":"#framework-de-avaliacao","text":"A minha an\u00e1lise sobre os dados da Stone segue duas grandes etapas . Primeiramente foi realizado uma an\u00e1lise mais descritiva dos dados, com o objetivo de conhecer detalhes dos dados e poss\u00edveis inconsist\u00eancias. E posteriormente, uma an\u00e1lise mais direcionada para resolu\u00e7\u00e3o do problema. Nas valida\u00e7\u00f5es iniciais ser\u00e1 registrado a etapa descritiva, e em fluxo de etapas realizadas ser\u00e1 registrado os passos para chegar na melhor curva de acionamento do cliente.","title":"Framework de Avalia\u00e7\u00e3o"},{"location":"#validacoes-iniciais","text":"Como dito anteriormente, nesta etapa foi realizada uma an\u00e1lise descritiva das bases que est\u00e3o dispon\u00edveis para trabalhar. Nesse caso n\u00f3s tempos: portfolio_clientes portfolio_comunicados portfolio_geral portfolio_tpv # libs import pandas as pd import numpy as np import matplotlib.pyplot as plt from src.make_plot import plot # matplotlib configs plt . rcParams [ \"figure.figsize\" ] = ( 12 , 7 ) plt . rcParams [ \"figure.dpi\" ] = 200 # data clients = pd . read_csv ( \"data/portfolio_clientes.csv\" ) tpv = pd . read_csv ( \"data/portfolio_tpv.csv\" ) comunicados = pd . read_csv ( \"data/portfolio_comunicados.csv\" ) geral = pd . read_csv ( \"data/portfolio_geral.csv\" )","title":"Valida\u00e7\u00f5es iniciais"},{"location":"#1-portfolio_clientes","text":"Aqui encontramos todas as informa\u00e7\u00f5es relacionadas aos dados cadastrais dos clientes que contratam empr\u00e9stimos, assim como dados geogr\u00e1ficos e segmentos do neg\u00f3cio. A base deveria possuir como registro \u00fanico, a coluna de nr_documento , por\u00e9m foi visto que existem registros duplicados. Devido a essa inconsist\u00eancia, os contratos relacionadas aos nr_documento duplicados n\u00e3o ser\u00e3o considerados. Vamos ent\u00e3o calcular o total de clientes registrados: # total de clientes clients_wo_duplicate = clients . drop_duplicates ([ \"nr_documento\" ]) clients_wo_duplicate . shape ( 14265 , 6 ) Dessa forma, possu\u00edmos um total de 14.265 clientes na base. Esse valor n\u00e3o reflete diretamente na quantidade de contrato j\u00e1 que cada cliente pode possuir mais de um contrato. Apenas a n\u00edvel j\u00e1 realizarmos um comparativo, puxamos tamb\u00e9m o n\u00famero de contratos registrados na base: # total de clientes geral . drop_duplicates ( \"contrato_id\" ) . shape ( 14756 , 22 ) Trazendo a informa\u00e7\u00e3o da tabela portfolio_geral vemos que existe um n\u00famero levemente maior, 14.756 contratos , registrados. Essa base \u00e9 composta de mais algumas vari\u00e1veis categ\u00f3ricas, e saber como as mesmas est\u00e3o distribu\u00eddas pode nos trazer bons insights para an\u00e1lises futuras mais complexas.","title":"1. portfolio_clientes"},{"location":"#distribuicao-de-tipo-da-empresa","text":"# getthering the data to plot counts = clients . value_counts ( \"tipo_empresa\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"tipo_empresa\" ] . values # this function is part of the code on src directory plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o do Tipo de Empresa\" , \"Tipos de Empresas\" , \"Quantidade de Clientes em cada Empresa\" , False ) A maioria dos clientes est\u00e3o cadastrados como Pessoa Jur\u00eddica , e bem pouco como Pessoal F\u00edsica .","title":"Distribui\u00e7\u00e3o de Tipo da Empresa"},{"location":"#distribuicao-dos-estados","text":"counts = clients . value_counts ( \"estado\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"estado\" ] . values # this function is part of the code on src directory plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o de quantidade de clientes em cada Estado.\" , \"Estados brasileiros\" , \"Quantidade de clientes em cada Estado\" , False ) Aqui vemos uma grande maioria dos clientes pertecentes ao Estado de S\u00e3o Paulo , seguido pelo Rio de Janeiro . A diferen\u00e7a dentre os outros Estados vai decaindo gradualmente.","title":"Distribui\u00e7\u00e3o dos Estados"},{"location":"#distribuicao-dos-segmentos-e-subsegmentos","text":"counts = clients . value_counts ( \"segmento\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"segmento\" ] . values # this function is part of the code on src directory plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o de quantidade de clientes em cada segmento.\" , \"Segmentos\" , \"Quantidade de clientes em cada segmento\" , True ) Quando olhamos para os segmentos, \u00e9 poss\u00edvel ver uma predomin\u00e2ncia do setor de Alimenta\u00e7\u00e3o , seguido de Verejo , Bens Dur\u00e1veis , Servi\u00e7os e Supermercado/Farm\u00e1cia , com uma minoria em outras 4 categorias. counts = clients . value_counts ( \"subsegmento\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"subsegmento\" ] . values # this function is part of the code on src directory plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o de quantidade de clientes em cada subsegmento.\" , \"Subsegmentos\" , \"Quantidade de clientes em cada subsegmento\" , True ) J\u00e1 nos Subsegmentos, temos duas grandes representatividades, Alimenta\u00e7\u00e3o R\u00e1pida e Lojas Diversas. De Outros em diante, a frequ\u00eancia de clientes vai caindo gradualmente. Um ponto interessante a se observar \u00e9 que a maior quantidade de clientes est\u00e1 relacionada a Alimenta\u00e7\u00e3o , seja diretamente, como indiretamente dentro dos top 5, como: Bares e Restaurantes e Supermercados. Interessante a se notar tamb\u00e9m \u00e9 que temos a presen\u00e7a de um subsegmento chamado None como uma string v\u00e1lida, quando na verdade categoriza um missing value e n\u00e3o um subsegmento de fato.","title":"Distribui\u00e7\u00e3o dos Segmentos e Subsegmentos"},{"location":"#2-portfolio_tpv","text":"Essa tabela tr\u00e1s toda a informa\u00e7\u00e3o de Total Paid Value que representa o valor transacionado no dia por cada um dos clientes pela maquininha Stone. Aqui buscou-se tamb\u00e9m pela contagem de quantos nr_documento existem na tabela, chegando no valor de 14.259 clientes . tpv . drop_duplicates ( \"nr_documento\" ) . shape ( 14259 , 4 ) Veja que aqui n\u00f3s temos uma diverg\u00eancia em rela\u00e7\u00e3o \u00e0 quantidade de nr_documento presente na tabela portfolio_geral . Um total de 6 clientes n\u00e3o possuem registro de TPV, por\u00e9m, quando analisados na portfolio_geral vemos que os mesmos possuem valor de pagamento realizado, caracterizados pela presen\u00e7a de m\u00e9dia. nr_documento_tpv = tpv . drop_duplicates ( \"nr_documento\" )[ \"nr_documento\" ] . to_list () geral_for_not_tpv = geral [ ~ geral [ \"nr_documento\" ] . isin ( nr_documento_tpv )] geral_for_not_tpv [[ \"vlr_desembolsado\" , \"vlr_pgto_realizado\" ]] . describe () stats vlr_desembolsado vlr_pgto_realizado count 3938.00 3938.00 mean 11854.92 11.86 std 7246.29 176.45 min 384.75 0.00 25% 3078.00 0.00 50% 17100.00 0.00 75% 17100.00 0.00 max 20520.00 5969.64 Olhando um pouco mais no detalhe, \u00e9 poss\u00edvel ver que alguns dos clientes de fato n\u00e3o tiveram nenhum pagamento realizado, o que justifica o mesmo n\u00e3o possuir registro na tabela portfolio_tpv . geral_for_not_tpv . groupby ([ \"nr_documento\" ])[ \"vlr_pgto_realizado\" ] . agg ([ \"mean\" ]) nr_documento mean 0a6b34a6b108015777d83b1023d43342 23.359794 1eb088b95b56970c880030922dce1c85 17.530547 69116fe5b82f759fd2f295f1daa92ecf 0.000000 6ef839f0201c6295072e45a95eb34466 10.529941 e6addfdeb74a038bb5f7149c7cfb1290 17.557847 ec55907309c0e6195675cb786f7d7242 0.000000 Analisar o TPV como um todo n\u00e3o \u00e9 muito conclusivo, temos valores muito dispersos de valor transacionado diariamente, assim como de m\u00e1ximos e m\u00ednimos. tpv [[ \"qtd_transacoes\" , \"vlr_tpv\" ]] . describe () . apply ( lambda s : s . apply ( \" {0:.5f} \" . format )) stats dt_transacao qtd_transacoes vlr_tpv count 4408597.00 4408597.00 4408597.00 mean 20204274.56 15.91 887.50 std 4656.32 26.15 1664.21 min 20200101.00 -2.00 -125000.00 25% 20200624.00 3.00 175.00 50% 20201030.00 7.00 430.00 75% 20210226.00 18.00 978.00 max 20210630.00 1245.00 176880.93 Portanto, como temos os dados cadastrais dos clientes, \u00e9 mais interessante levar isso em considera\u00e7\u00e3o na hora fazer a an\u00e1lise. Vamos primeiramente juntar as bases: tpv_merged = tpv . merge ( right = clients , on = \"nr_documento\" , how = \"inner\" ) Analisando primeiramente o TPV para segmento e subsegmento, temos: tpv_sum_segment = tpv_merged . groupby ([ \"segmento\" ])[ \"vlr_tpv\" ] . agg ([ \"sum\" ]) . reset_index () tpv_sum_segment [ \"prop\" ] = tpv_sum_segment [ \"sum\" ] / tpv_sum_segment [ \"sum\" ] . sum () tpv_sum_subsegment = tpv_merged . groupby ([ \"subsegmento\" ])[ \"vlr_tpv\" ] . agg ([ \"sum\" ]) . reset_index () tpv_sum_subsegment [ \"prop\" ] = tpv_sum_subsegment [ \"sum\" ] / tpv_sum_subsegment [ \"sum\" ] . sum () tpv_sum_segment . sort_values ( \"prop\" , ascending = False ) segmento sum prop Alimenta\u00e7\u00e3o 1.473060e+09 0.372553 Bens dur\u00e1veis 7.342777e+08 0.185707 Varejo 5.917481e+08 0.149660 Supermercado/Farm\u00e1cia 4.799287e+08 0.121379 Servi\u00e7os 4.409396e+08 0.111518 Posto 8.120651e+07 0.020538 Servi\u00e7os recorrentes 6.621661e+07 0.016747 Viagens e entretenimento 4.964331e+07 0.012555 Outros 3.694166e+07 0.009343 Aqui vemos que a ordem muda um pouco, onde Varejo perde posi\u00e7\u00e3o para Bens Dur\u00e1veis e Servi\u00e7os para Supermercado/Farm\u00e1cia tpv_sum_subsegment . sort_values ( \"prop\" , ascending = False ) subsegmento sum prop Alimenta\u00e7\u00e3o R\u00e1pida 7.782735e+08 0.196834 Lojas Diversas 6.846034e+08 0.173144 Supermercados 4.372940e+08 0.110596 Bares e Restaurantes 3.662225e+08 0.092622 Outros 3.469090e+08 0.087737 Automotivo 2.711954e+08 0.068588 Sa\u00fade 2.537558e+08 0.064178 Material de Constru\u00e7\u00e3o 2.178250e+08 0.055090 Vestu\u00e1rio 2.070685e+08 0.052370 Atacadistas Gerais 1.396476e+08 0.035318 Postos de Gasolina 8.112604e+07 0.020518 Educa\u00e7\u00e3o 4.507371e+07 0.011400 None 4.076986e+07 0.010311 Lazer & Turismo 3.600002e+07 0.009105 Atacadista de Alimento 2.495160e+07 0.006311 Academias 2.324615e+07 0.005879 Em rela\u00e7\u00e3o ao subsegmento, vemos que houve uma troca de posi\u00e7\u00e3o entre Supermercados e Outros . Com essas duas bases podemos buscar por sasonalidade nos dados e melhor compreender como cada neg\u00f3cio oscila ao passar do tempo. Para isso, iremos utilizar uma vis\u00e3o de heatmap variando na horizontal o meses do ano, e na vertical todos os nossos Estados, e o valor mapeado no heatmap \u00e9 o somat\u00f3rio do TPV naquele espec\u00edfico grupo. Vamos criar as vari\u00e1veis temporais que iremos precisar e agroupar os dados que ser\u00e3o utilizados para o heatmap: tpv_merged [ \"dt_transacao\" ] = pd . to_datetime ( tpv_merged [ \"dt_transacao\" ], format = \"%Y%m %d \" ) tpv_merged [ \"month\" ] = tpv_merged [ \"dt_transacao\" ] . dt . month tpv_merged [ \"year\" ] = tpv_merged [ \"dt_transacao\" ] . dt . year # grouping df = ( tpv_merged . groupby ([ \"segmento\" , \"estado\" , \"year\" , \"month\" ])[ \"vlr_tpv\" ] . agg ([ \"sum\" ]) . reset_index () ) # labels for the plot segmentos = list ( df [ \"segmento\" ] . unique ()) monhts = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ] # this code will generate all heatmaps for segmento in segmentos : df_segmento = df [( df [ \"year\" ] == 2020 ) & ( df [ \"segmento\" ] == segmento )] estados = list ( df_segmento [ \"estado\" ] . unique ()) df_heatmap = df_segmento . drop ([ \"segmento\" ], axis = 1 ) . pivot_table ( values = \"sum\" , index = [ \"estado\" ], columns = [ \"month\" ] ) fig , ax = plt . subplots () im , cbar = plot . heatmap ( df_heatmap , estados , monhts , ax = ax , cmap = \"YlGn\" , cbarlabel = \"Total TPV\" ) # Number of accent colors in the color scheme plt . title ( segmento ) fig . tight_layout () plt . show () Como temos diferentes tipos de segmentos, achei mais prudente trazer apenas os Top 3 segmentos de maior TPV para interpretarmos os heatmaps. O que vemos aqui \u00e9 que o valor do total de TPV transacionado ao longo do ano de 2020 foram bem semelhantes, com predom\u00ednio de S\u00e3o Paulo e Rio de Janeiro (por serem os Estados com mais clientes) e a predomin\u00e2ncia tamb\u00e9m do per\u00edodo do segundo trimestre ter sido um per\u00edodo de baixa e que provavelmente deve ter impactado o valor pago nas maquininhas. O Subsegmento tamb\u00e9m demonstrou comportamento semelhante ao considerar os Top 3 pelo valor do TPV. A \u00fanica diferen\u00e7a \u00e9 que acabamos tendo uma variabilidade maior entre os Estados e ao longo do ano para subsegmentos menores, como Academia e Atacadista de Alimento . Acredito que tal comportamento deveria ser levado em considera\u00e7\u00e3o ao estabelecer uma r\u00e9gua de acionamento. Pois at\u00e9 mesmo, as cidades dentro de Estados maiores, como S\u00e3o Paulo e Rio de Janeiro tamb\u00e9m podem apresentar realidades diferentes por Segmentos e Subsegmentos.","title":"2. portfolio_tpv"},{"location":"#3-portfolio_comunicados","text":"Essa tabela possui todo o dado de quem foi acionado no seu hist\u00f3rico de empr\u00e9stimo de linha de cr\u00e9dito. Ao longo desse per\u00edodo, 403.704 acionamentos foram realizados, onde 47.35% sofreram algum problema e acabaram por n\u00e3o serem entregues, 34% foram de fato entregues, 17.6 foram lidas e 0.08 foram respondidas. comunicados . value_counts ( \"status\" , normalize = True ) Tamb\u00e9m observou-se que os acionamentos s\u00e3o realizados por dois canais, mensagens diretas e email . Por\u00e9m, ao analisar os subtotais dentro de cada status, vemos que n\u00e3o existe prefer\u00eancia entre os tipos canais utilizados comunicados . value_counts ([ \"status\" , \"tipo_acao\" ], normalize = True ) . reset_index () status tipo_acao prop NAO ENTREGUE HSM 0.236836 NAO ENTREGUE EMAIL 0.236754 ENTREGUE HSM 0.171046 ENTREGUE EMAIL 0.170637 LIDO HSM 0.088706 LIDO EMAIL 0.088091 RESPONDIDO EMAIL 0.004518 RESPONDIDO HSM 0.003413 Todos esses acionamentos foram realizados para um total de 12.202 contratos , o que representa aproximadamente 83% da base . Um n\u00famero bastante alto de clientes que apresentaram algum tipo de inadipl\u00eancia. E por fim, pelo gr\u00e1fico abaixo, podemos ver que 52% das campanhas realizadas foram de observa\u00e7\u00e3o , seguido de parcelamento e boleto quitado. counts = comunicados . value_counts ( \"acao\" ) . reset_index () y = counts [ 0 ] . values x = counts [ \"acao\" ] . values plot . make_beautiful_bar_plots ( x , y , \"Distribui\u00e7\u00e3o do Tipo de A\u00e7\u00e3o\" , \"Tipos de A\u00e7\u00f5es\" , \"Quantidade de A\u00e7\u00f5es realizadas\" , False )","title":"3. portfolio_comunicados"},{"location":"#4-portfolio_geral","text":"Essa \u00e9 a maior tabela que temos e a mesma tem todo o hist\u00f3rico de cada um dos mais de 12k de contratos registrados. Com essa tabela farei algumas sumariza\u00e7\u00f5es e cria\u00e7\u00e3o de m\u00e9tricas envolvendo o dsp (Dias Corridos sem Pagamento) e o dspp (Dias Corridos sem Pagamento do Principal), que ser\u00e3o descritas na pr\u00f3xima do framework de avalia\u00e7\u00e3o. Essa tabela, em conjunto com dados de todas as outras, ir\u00e1 nos auxiliar a responder o nosso problema principal: Qual \u00e9 a quantidade ideal de vezes que acionamos um cliente? .","title":"4. portfolio_geral"},{"location":"#fluxo-de-etapas-realizadas","text":"Tudo que foi feito at\u00e9 o momento pode ser encontrado no notebook describe.ipynb . Daqui pra frente iremos consumir dos notebooks final_composition.ipynb e analysis.ipynb . Onde o primeiro \u00e9 o respons\u00e1vel por juntar as tabelas e o segundo possui o desenrolar da an\u00e1lise. Como output, o notebook final_composition.ipynb ir\u00e1 nos fornecer um dataset que permitir\u00e1 uma an\u00e1lise mais robusta e a entender a melhor curva de acionamento ao cliente.","title":"Fluxo de etapas realizadas"},{"location":"#1-merge-e-criacao-de-features","text":"Primeiramente, vamos precisar trazer todos as nossas tabelas e as libs que ser\u00e3o utilizadas. import pandas as pd from src.feature_engineering import features import numpy as np clients = pd . read_csv ( \"data/portfolio_clientes.csv\" ) tpv = pd . read_csv ( \"data/portfolio_tpv.csv\" ) comunicados = pd . read_csv ( \"data/portfolio_comunicados.csv\" ) geral = pd . read_csv ( \"data/portfolio_geral.csv\" ) Aqui vamos utilizar um m\u00f3dulo de feature engineering que criei para nos auxiliar na cria\u00e7\u00e3o das features de uma forma mais modular e clean. Detalhes sobre essas etapas espec\u00edficas de feature engineering v\u00e3o ser retratadas em uma p\u00e1gina b\u00f4nus da documenta\u00e7\u00e3o.","title":"1. Merge e Cria\u00e7\u00e3o de Features"},{"location":"#geral-comunicados","text":"Como visto anteriormente, nem todos os clientes receberam acionamento, por isso, vamos precisar filtrar a base para contemplar apenas os casos que possuem acionamento. # contratos \u00fanicos na tabela de comunicados unique_contratos = comunicados [ \"contrato_id\" ] . unique () # filtrando por esses contratos geral_comunicados = geral [ geral [ \"contrato_id\" ] . isin ( unique_contratos )] # como cada contrato possui mais de um acionamento, para evitar duplicidades # vamos agregar em uma lista os duplicados. comunicados_grouped = ( comunicados . groupby ([ \"contrato_id\" , \"dt_ref_portfolio\" , \"data_acao\" ])[ [ \"tipo_acao\" , \"status\" , \"acao\" ] ] . agg ( list ) . reset_index () ) geral_comunicados_grouped = geral_comunicados . merge ( right = comunicados_grouped , how = \"left\" , on = [ \"contrato_id\" , \"dt_ref_portfolio\" ] ) Veja que estamos utilizando um left-join aqui por que a tabela portfolio_geral possui valores di\u00e1rios, e nem todos os dias ocorreram acionamentos, por\u00e9m, ter essa informa\u00e7\u00e3o \u00e9 muiti \u00fatil para as pr\u00f3ximas etapas. Por fim, vamos realizar um sort nos dados, para garantir que o dsp e o dspp estar\u00e3o na ordem correta mais pra frente, quando formos criar as novas features. geral_and_comunicados_sorted_df = geral_comunicados_grouped . sort_values ( [ \"contrato_id\" , \"dt_ref_portfolio\" ] )","title":"Geral + Comunicados"},{"location":"#criacao-de-features-de-dsp-e-dspp","text":"Aqui est\u00e1 uma etapa muito importante do fluxo da an\u00e1lise dos dados. Uma informa\u00e7\u00e3o que precisamos ter para prosseguir com a an\u00e1lise \u00e9 se os acionamentos foram efetivos para retornar o cliente para a utiliza\u00e7\u00e3o da maquininha da Stone. Tal feature aqui vai ser entendidad da seguinte forma: Se o cliente retornou sua utiliza\u00e7\u00e3o da maquininha dentro de per\u00edodo que antecede o acionamento subsequente, configura SUCESSO. Com isso em mente, criou-se o m\u00f3dulo de feature_engineering , onde parte das fun\u00e7\u00f5es contemplam exatamente esse c\u00e1lculo. Os dois blocos de c\u00f3digo abaixo apresentam como as fun\u00e7\u00f5es s\u00e3o aplicadas por interm\u00e9dio do pandas . # para o dsp contrato_dsp_features = ( geral_and_comunicados_sorted_df . groupby ([ \"contrato_id\" ])[ \"dsp\" ] . agg ( [ features . total_success_dsp5 , features . total_success_dsp10 , features . total_success_dsp15 , features . total_success_dsp30 , features . total_success_dsp60 , features . total_success_dsp90 , features . prop_success_dsp5 , features . prop_success_dsp10 , features . prop_success_dsp15 , features . prop_success_dsp30 , features . prop_success_dsp60 , features . prop_success_dsp90 , ] ) . reset_index () ) # para o dspp contrato_dspp_features = ( geral_and_comunicados_sorted_df . groupby ([ \"contrato_id\" ])[ \"dspp\" ] . agg ( [ features . total_success_dspp15 , features . total_success_dspp30 , features . total_success_dspp45 , features . prop_success_dspp15 , features . prop_success_dspp30 , features . prop_success_dspp45 , ] ) . reset_index () ) # merging os dois sets de features criadas contrato_dsp_dspp = contrato_dsp_features . merge ( right = contrato_dspp_features , on = \"contrato_id\" , how = \"inner\" ) Veja que, aqui temos n\u00e3o s\u00f3 o total de sucesso em cada um dos acionamentos, mas tamb\u00e9m uma propor\u00e7\u00e3o entre: acionamentos_com_sucesso / total_de_acionamentos Por fim, para juntar esses valores num score mais representativo , vamos calcular as m\u00e9dias das propor\u00e7\u00f5es, considerando os casos onde de fato o cliente recebeu acionamento. means_dsp = [] means_dspp = [] for i , row in contrato_dsp_dspp . iterrows (): means_dsp . append ( np . nanmean ( row [ 7 : 13 ])) means_dspp . append ( np . nanmean ( row [ 16 : 19 ])) contrato_dsp_dspp [ \"score_dsp\" ] = means_dsp contrato_dsp_dspp [ \"score_dspp\" ] = means_dspp","title":"Cria\u00e7\u00e3o de features de DSP e DSPP"},{"location":"#entregou-nao-entregou-leu","text":"Um outro ponto muito importante que devemos levar em considera\u00e7\u00e3o \u00e9 se o cliente de fato leu a mensagem ou se o mesmo at\u00e9 mesmo recebeu, j\u00e1 que vimos que o n\u00famero de clientes que n\u00e3o recebem o acionamento (quando deveria ter recebido) \u00e9 bastante alto, cerca de 47%. acionamentos_delivery = ( geral_and_comunicados_sorted_df . groupby ([ \"contrato_id\" ])[ \"status\" ] . agg ([ features . get_entregue , features . get_lido , features . get_nao_entregue ]) . reset_index () ) # merge para adicionar ao nosso dataset final as features de entregue, n\u00e3o entregue e lido contrato_dsp_dspp_qtd_acoes = contrato_dsp_dspp . merge ( right = acionamentos_delivery , how = \"inner\" , on = \"contrato_id\" ) Aqui, tamb\u00e9m estamos utilizando o m\u00f3dulo de feature_engineering .","title":"Entregou? N\u00e3o Entregou? Leu?"},{"location":"#valor-devedor-esperado","text":"Essa feature pode nos ajudar a entender se existe alguma rela\u00e7\u00e3o entre o valor total de empr\u00e9stimo do cr\u00e9dito com alguma outra feature que iremos trazer para compor a an\u00e1lise final. Como essa informa\u00e7\u00e3o n\u00e3o estava expl\u00edcita, resolvi considerar o valor devedor esperado no primeiro dia do contrato como valor de empr\u00e9stimo daquele cliente. # features de vlr_saldo_devedor vlr_saldo_devedor_inicial = geral_and_comunicados_sorted_df . drop_duplicates ( [ \"contrato_id\" ] )[[ \"contrato_id\" , \"vlr_saldo_devedor_esperado\" ]] c_dsp_dspp_qtd_acoes_devedor = contrato_dsp_dspp_qtd_acoes . merge ( right = vlr_saldo_devedor_inicial , how = \"inner\" , on = \"contrato_id\" )","title":"Valor devedor esperado"},{"location":"#trazendo-dados-cadastrais","text":"Nossa tabela at\u00e9 o momento possui o contrato_id e algumas features que coletamos. Por\u00e9m, para cruzar com os dados cadastrais dos clientes, vamos precisar tamb\u00e9m do nr_documento . Para isso criamos uma tabela intermedi\u00e1ria que vai nos ajudar a trazer os dados dos clientes para essa base. # tabela intermedi\u00e1ria x_contrato_id_nr_documento = geral_and_comunicados_sorted_df . drop_duplicates ( [ \"contrato_id\" , \"nr_documento\" ] )[[ \"contrato_id\" , \"nr_documento\" ]] # trazendo os nr_documentos c_dsp_dspp_qtd_acoes_devedor_w_doc = c_dsp_dspp_qtd_acoes_devedor . merge ( right = x_contrato_id_nr_documento , how = \"inner\" , on = \"contrato_id\" ) # alguns nr_documentos est\u00e3o duplicados por que o cliente # pode ter mais de uma loja ou em diferentes regi\u00f5es # por isso estou agrupando os casos onde acontece duplicatas clientes_unique_nr_doc = ( clientes . groupby ( \"nr_documento\" )[ [ \"tipo_empresa\" , \"cidade\" , \"estado\" , \"subsegmento\" , \"segmento\" ] ] . agg ( lambda x : list ( x ) if len ( x ) > 1 else x ) . reset_index () ) # etapa final, de fato trazendo os dados dos clientes pra base c_dsp_dspp_qtd_acoes_devedor_w_doc_and_clients = c_dsp_dspp_qtd_acoes_devedor_w_doc . merge ( right = clientes_unique_nr_doc , on = \"nr_documento\" , how = \"inner\" )","title":"Trazendo dados cadastrais"},{"location":"#tpv","text":"Agora, vamos trazer a informa\u00e7\u00e3o do TPV, sumerizado por cliente. Aqui teremos: m\u00ednimo m\u00e1ximo m\u00e9dia mediana soma Para quantidade de transa\u00e7\u00e3o realizada no dia e para o valor do tpv. qtd_trans_tpv = tpv . groupby ( \"nr_documento\" )[[ \"qtd_transacoes\" , \"vlr_tpv\" ]] . agg ( [ \"mean\" , \"min\" , \"max\" , np . median , \"sum\" ] ) final_df = c_dsp_dspp_qtd_acoes_devedor_w_doc_and_clients . merge ( right = qtd_trans_tpv , how = \"left\" , on = \"nr_documento\" ) # saving our dataset final_df . to_csv ( \"data/to_analysis.csv\" , index = False )","title":"+ TPV"},{"location":"#2-analise-exploratoria-dataset-final","text":"Agora, com nosso novo dataset, n\u00f3s temos cerca de 41 features as quais podemos utilizar para chegar a conclus\u00e3o de qual a melhor curva de acionamento do cliente. Por quest\u00e3o de tempo e disponibilidade, irei focar em duas an\u00e1lises na tentativa de encontrar algum padr\u00e3o que nos direcione para nosso objetivo. A primeira ser\u00e1 uma an\u00e1lise do score_dsp e score_dspp com features de segmento e subsegmento, e em segundo, do score_dsp e score_dspp com features que indicam se as mensagens foram de fato entregues, lidas ou se simplesmente n\u00e3o foram entregues. Vale pontuar aqui que esses scores foram calculado, tirando a m\u00e9dia da porcentagem de sucesso ao aplicar uma campanha. E tamb\u00e9m que poderia muito bom cada uma dessas campanhas possuirem um peso espec\u00edfico, por\u00e9m nesse primeiro momento vamos considerar todas as campanhas com o mesmo peso.","title":"2. An\u00e1lise Explorat\u00f3ria (Dataset final)"},{"location":"#dsp-e-dspp-pelo-segmento-e-subsegmento","text":"Vamos iniciar filtrando o dados para retornar apenas os dados que vamos utilizar. # intially by the dsp df_filtered = df [[ \"nr_documento\" , \"score_dsp\" , \"segmento\" ]][ ( df [ \"segmento\" ] . isin ( segmentos )) & ( df [ \"subsegmento\" ] . isin ( subsegmentos )) ] Aqui tamb\u00e9m estamos filtrando pelas categorias nos segmentos e subsegmentos, removendo os casos de nr_documento duplicado. Vamos ent\u00e3o analisar um boxplot para cada uma das categorias e ter uma vis\u00e3o de como est\u00e3o distribu\u00eddos os scores. fig = px . box ( df_filtered , x = \"segmento\" , y = \"score_dsp\" ) fig . show () Com esse gr\u00e1fico, podemos ver que existe uma vari\u00e2ncia alta nos dados, e independemente do segmento, existem valores que v\u00e3o do 0 at\u00e9 o 1. Por\u00e9m, ao olharmos para a mediana, vemos que Servi\u00e7os recorrentes possuem os valores mais altos desse do score_dsp . Agora, para o subsegmento: # intially by the dsp df_filtered = df [[ \"nr_documento\" , \"score_dsp\" , \"subsegmento\" ]][ ( df [ \"segmento\" ] . isin ( segmentos )) & ( df [ \"subsegmento\" ] . isin ( subsegmentos )) ] fig = px . box ( df_filtered , x = \"subsegmento\" , y = \"score_dsp\" ) fig . show () O mesmo comportamento do anteior pode ser visto aqui. Os valores de mediana mais altos est\u00e3o por conta do subsegmento Educa\u00e7\u00e3o . E setores relacionados a Alimenta\u00e7\u00e3o, direta ou indiretamente, possuem valores em torno de 0.5 de mediana no score_dsp . Vamos visualizar o mesmo para o dspp: df_filtered = df [[ \"nr_documento\" , \"score_dspp\" , \"segmento\" , \"subsegmento\" ]][ ( df [ \"segmento\" ] . isin ( segmentos )) & ( df [ \"subsegmento\" ] . isin ( subsegmentos )) ] fig = px . box ( df_filtered , x = \"segmento\" , y = \"score_dspp\" ) fig . show () fig = px . box ( df_filtered , x = \"subsegmento\" , y = \"score_dspp\" ) fig . show () Pelo que vemos aqui, tanto para segmento, quanto para subsegmento, apesar de termos valores oscilando de 0 a 1 em todas as categorias, as medianas aqui s\u00e3o bem menores que no dsp. Podemos inferir aqui que o score n consegue ser explicado apenas pelo segmento e subsegmento, sendo necess\u00e1rio mais vari\u00e1veis, vamos seguir para a utiliza\u00e7\u00e3o das features de entrega do acionamento.","title":"DSP e DSPP pelo Segmento e Subsegmento"},{"location":"#dsp-e-dspp-pela-entrega-do-acionamento","text":"Estou considerando aqui a entrega do acionamento pelas features de: ENTREGUE N\u00c3O ENTREGUE LIDO E, constru\u00edndo os boxplots novamente, temos: df_filtered = df [ [ \"nr_documento\" , \"score_dsp\" , \"get_entregue\" , \"get_nao_entregue\" , \"get_lido\" ] ][( df [ \"segmento\" ] . isin ( segmentos )) & ( df [ \"subsegmento\" ] . isin ( subsegmentos ))] # melting the data df_filtered_melted = df_filtered . melt ( id_vars = [ \"nr_documento\" , \"score_dsp\" ], value_vars = [ \"get_entregue\" , \"get_nao_entregue\" , \"get_lido\" ], var_name = \"acionamento\" , ) fig = px . box ( df_filtered_melted , x = \"acionamento\" , y = \"score_dsp\" ) fig . show () Curiosamente, apesar de nada muito \u00fatil, n\u00e3o conseguimos ver nenhuma rela\u00e7\u00e3o entre as vari\u00e1veis de acionamento e os scores para o dsp. O mesmo foi feito para o dspp, mas obtive o mesmo resultado, n\u00e3o demonstrando nada muito informativo, por isso o gr\u00e1fico n\u00e3o foi plotado aqui.","title":"DSP e DSPP pela Entrega do Acionamento"},{"location":"#3-dashboarding","text":"Ap\u00f3s realizada a an\u00e1lise acima, cheguei a conclus\u00e3o que a identifica\u00e7\u00e3o das melhores features que podem nos ajudar a explicar a melhor curva de acionamento do cliente \u00e9 um trabalho que existe mais conhecimento de neg\u00f3cio e mais tentativa e erro. Para otimizar essa an\u00e1lise, irei apresentar um dashboard com alguns filtros e visualiza\u00e7\u00f5es sobre a mediana do sucesso em cada uma das campanhas, por cliente. Dessa forma conseguimos empoderar os usu\u00e1rios com dados e otimizar a tomada de decis\u00e3o. O que teremos de c\u00f3digo dentro do dash ser\u00e1 como o exibido abaixo: # sele\u00e7\u00e3o de filtros filtro_estado = \"SP\" filtro_cidade = \"S\u00e3o Paulo\" prop_columns = [ column for column in df . columns if column . startswith ( \"prop_\" )] # aplica\u00e7\u00e3o dos filtros df_filtered = df [( df [ \"estado\" ] == filtro_estado ) & ( df [ \"cidade\" ] == filtro_cidade )][ prop_columns ] df_melted = df_filtered . melt ( value_vars = prop_columns , var_name = \"props\" ) to_plot = df_melted . groupby ([ \"props\" ])[ \"value\" ] . agg ( np . nanmedian ) . reset_index () # transformando no tipo Categories, para ordenar o plot prop_categories = CategoricalDtype ( [ \"prop_success_dsp5\" , \"prop_success_dsp10\" , \"prop_success_dsp15\" , \"prop_success_dsp30\" , \"prop_success_dsp60\" , \"prop_success_dsp90\" , \"prop_success_dspp15\" , \"prop_success_dspp30\" , \"prop_success_dspp45\" , ], ordered = True ) to_plot [ \"props\" ] = to_plot [ \"props\" ] . astype ( prop_categories ) to_plot_sorted = to_plot . sort_values ( \"props\" ) # plot fig = px . bar ( to_plot_sorted , x = \"props\" , y = \"value\" ) fig . show () A interpreta\u00e7\u00e3o \u00e9 que, para esse conjunto de clientes, filtrados para S\u00e3o Paulo e tamb\u00e9m cidade de S\u00e3o Paulo, tivemos uma convers\u00e3o alta para o dsp5, e a mesma foi declinando at\u00e9 o dsp30. J\u00e1 para o DSPP, tivemos uma convers\u00e3o bem baixa, para os casos onde houve esse tipo de acionamento, chegando a 20%.","title":"3. Dashboarding"},{"location":"#conclusoes-e-insights","text":"Optei nesse projeto por uma abordagem mais simples, que consistiu uma an\u00e1lise descritiva seguido de uma sumeriza\u00e7\u00e3o dos dados, levando em considera\u00e7\u00e3o cada um dos contratos e clientes presentes na base. O objetivo foi de conseguir insights iniciais que possam nortear melhor a tomada de decis\u00e3o sobre qual a melhor curva de acionamento do cliente. Nesse quesito, cheguei em um score de sucesso de cada um dos acionamentos direcionados ao cliente. Por\u00e9m, esse score sozinho n\u00e3o \u00e9 suficiente para obtermos a melhor curva de acionamento, e o mesmo precisa ser cruzado com outras features ( segunda etapa de an\u00e1lise do projeto ). Ao final dessa segunda etapa, nosso dataset possuia 41 features, e isso foi inevit\u00e1vel dada a complexidade e nuances presentes no dados. Devido a isso, acredito que simples an\u00e1lises descritivas esbarram na limita\u00e7\u00e3o de quantas vari\u00e1veis conseguimos visualizar e buscar por padr\u00f5es. O que deixa claro que esse \u00e9 um problema que pode ser facilmente abordado como um problema de recomenda\u00e7\u00e3o, com base em todas as features criadas nesse processo de an\u00e1lise de dados. Outro ponto importante \u00e9 trazer para esse problema o conceito de experimenta\u00e7\u00e3o , para possibilitar a utiliza\u00e7\u00e3o de modelos de Uplift , e assim saber de fato qual grupo de clientes devem ser abordados com os acionamentos. Infelizmente n\u00e3o consegui chegar a uma curva ideal, mas pude compreender que esse problema que pode ser abordado levando em considera\u00e7\u00e3o a realidade de cada cliente presente na base, deixando o atendimento mais personalisado. Gostaria de finalizar agradecendo a oportunidade fornecida pela Stone em estar participando e aprendendo muito com esse desafio.","title":"Conclus\u00f5es e Insights"},{"location":"data_dict/","text":"Dicion\u00e1rio de Dados Diagrama de Entidade-Relacionamento para demonstrar como as bases est\u00e3o relacionadas. portfolio_geral Feature Type Description contrato_id string Identificador \u00fanico do contrato de um cliente. dt_ref_portfolio date Data de refer\u00eancia da base de portfolio, representa a qual data um determinado registro/linha corresponde. safra date M\u00eas que o contrato foi originado (desembolsado). nr_documento string N\u00famero do documento do cliente. status_contrato string Estado atual do empr\u00e9stimo, podendo ser \"Desembolso confirmado (DisbursementConfirmed), Aceito (Accepted), Desembolso solicitado (DisbursementRequested), Ativo (Active) ou Quitado (Settled)\". dt_contrato date Data de cria\u00e7\u00e3o do empr\u00e9stimo. dt_desembolso date Data de desembolso do empr\u00e9stimo, onde de fato liberamos o valor tomado na conta do cliente. dt_vencimento date Data de vencimento do contrato. dt_wo date Data de write-off do contrato onde, caso o cliente n\u00e3o evolua com o pagamento, o contrato \u00e9 declarado como perda. prazo bigint Prazo/parcelas do contrato da linha de cr\u00e9dito, geralmente o prazo m\u00e9dio \u00e9 de 8 meses. vlr_desembolsado decimal (15,2) Valor contratado no empr\u00e9stimo. vlr_tarifa decimal (15,2) Tarifa de contrata\u00e7\u00e3o (representa 1% do valor do empr\u00e9stimo). vlr_pgto_esperado decimal (15,2) Representa o valor de pagamento esperado de um contrato dado suas caracter\u00edsticas. \u00c9 resultado do c\u00e1lculo (juros_diario + iof_diario, (dt_vencimento - dt_desembolso).days, vlr_desembolsado + vlr_tarifa + iof_inicial). juros_mes decimal (15,2) Taxa de juros ao m\u00eas (% am). juros_diario decimal (15,2) Taxa de juros nominal do contrato representada por valor di\u00e1rio = (1 + juros_mes) ^ (1 / 30) - 1. perc_retencao decimal (15,2) Percentual retido diariamente sobre as liquida\u00e7\u00f5es do cliente que ser\u00e1 revertida no pagamento do contrato dia ap\u00f3s dia (quando houver TPV). vlr_pgto_realizado decimal (15,2) Valor do pagamento di\u00e1rio contabilizado de um cliente. vlr_saldo_devedor decimal (15,2) Valor devedor atualizado atual do contrato, considerando pagamentos, ac\u00famulo de juros e encargos do mesmo. vlr_saldo_devedor_esperado decimal (15,2) Valor devedor esperado do contrato dado o ritmo de pagamento previsto para o mesmo, considerando pagamentos, acumulo de juros e encargos do mesmo. dspp bigint Contagem de dias consecutivos sem amortiza\u00e7\u00e3o do saldo principal. A contagem \u00e9 sempre zero quando h\u00e1 amortiza\u00e7\u00e3o do principal. Ent\u00e3o se um cliente esta h\u00e1 50 dias sem amortizar o m\u00ednimo e no dia seguinte amortiza o principal (qualquer valor), a contagem reseta. dsp bigint Contagem de dias consecutivos sem qualquer pagamento realizado. flag_transacao int Flag indicado se, na data de refer\u00eancia, houve algum valor transacionado na maquininha do cliente (TPV). portfolio_tpv Features Type Description nr_documento string N\u00famero do documento do cliente. dt_transacao string Campo que indica a data que houve algum valor transacionado na maquininha do cliente. qtd_transacoes bigint Campo que indica a quantidade de transa\u00e7\u00f5es registradas na maquininha do cliente em determinada data. vlr_tpv double Este campo se refere ao valor total transacionado na maquininha do cliente. portfolio_comunicados Features Type Description contrato_id string Identificador \u00fanico do contrato de um cliente. dt_ref_portfolio date Data de refer\u00eancia da base de portfolio, representa a qual data um determinado registro/linha corresponde. data_acao date A data que uma a\u00e7\u00e3o de comunica\u00e7\u00e3o foi executada e enviada para o cliente. tipo_acao string Qual \u00e9 o tipo da a\u00e7\u00e3o de comunica\u00e7\u00e3o executada. acao string Tipo da a\u00e7\u00e3o de comunica\u00e7\u00e3o utilizada pelo time de cobran\u00e7a para identificar qual foi a campanha executada. Representa em qual momento da r\u00e9gua determinado contrato esta. status string Status atualizado da comunica\u00e7\u00e3o enviada, podendo ser 'Enviado', 'N\u00e3o entregue', 'Lido', 'Respondido'. portfolio_clientes Features Type Description nr_documento string N\u00famero do documento do cliente. tipo_empresa string Representa o tipo de empresa que corresponde do cliente MEI, CPF, CNPJ. cidade string Cidade de origem do cliente que contratou o produto de cr\u00e9dito. estado string Estado de origem (UF) do cliente que contratou o produto de cr\u00e9dito. subsegmento string Micro segmento de neg\u00f3cio em que o cliente est\u00e1 inserido. segmento string Macro segmento de neg\u00f3cio em que o cliente est\u00e1 inserido.","title":"Dicion\u00e1rio de Dados"},{"location":"data_dict/#dicionario-de-dados","text":"Diagrama de Entidade-Relacionamento para demonstrar como as bases est\u00e3o relacionadas.","title":"Dicion\u00e1rio de Dados"},{"location":"data_dict/#portfolio_geral","text":"Feature Type Description contrato_id string Identificador \u00fanico do contrato de um cliente. dt_ref_portfolio date Data de refer\u00eancia da base de portfolio, representa a qual data um determinado registro/linha corresponde. safra date M\u00eas que o contrato foi originado (desembolsado). nr_documento string N\u00famero do documento do cliente. status_contrato string Estado atual do empr\u00e9stimo, podendo ser \"Desembolso confirmado (DisbursementConfirmed), Aceito (Accepted), Desembolso solicitado (DisbursementRequested), Ativo (Active) ou Quitado (Settled)\". dt_contrato date Data de cria\u00e7\u00e3o do empr\u00e9stimo. dt_desembolso date Data de desembolso do empr\u00e9stimo, onde de fato liberamos o valor tomado na conta do cliente. dt_vencimento date Data de vencimento do contrato. dt_wo date Data de write-off do contrato onde, caso o cliente n\u00e3o evolua com o pagamento, o contrato \u00e9 declarado como perda. prazo bigint Prazo/parcelas do contrato da linha de cr\u00e9dito, geralmente o prazo m\u00e9dio \u00e9 de 8 meses. vlr_desembolsado decimal (15,2) Valor contratado no empr\u00e9stimo. vlr_tarifa decimal (15,2) Tarifa de contrata\u00e7\u00e3o (representa 1% do valor do empr\u00e9stimo). vlr_pgto_esperado decimal (15,2) Representa o valor de pagamento esperado de um contrato dado suas caracter\u00edsticas. \u00c9 resultado do c\u00e1lculo (juros_diario + iof_diario, (dt_vencimento - dt_desembolso).days, vlr_desembolsado + vlr_tarifa + iof_inicial). juros_mes decimal (15,2) Taxa de juros ao m\u00eas (% am). juros_diario decimal (15,2) Taxa de juros nominal do contrato representada por valor di\u00e1rio = (1 + juros_mes) ^ (1 / 30) - 1. perc_retencao decimal (15,2) Percentual retido diariamente sobre as liquida\u00e7\u00f5es do cliente que ser\u00e1 revertida no pagamento do contrato dia ap\u00f3s dia (quando houver TPV). vlr_pgto_realizado decimal (15,2) Valor do pagamento di\u00e1rio contabilizado de um cliente. vlr_saldo_devedor decimal (15,2) Valor devedor atualizado atual do contrato, considerando pagamentos, ac\u00famulo de juros e encargos do mesmo. vlr_saldo_devedor_esperado decimal (15,2) Valor devedor esperado do contrato dado o ritmo de pagamento previsto para o mesmo, considerando pagamentos, acumulo de juros e encargos do mesmo. dspp bigint Contagem de dias consecutivos sem amortiza\u00e7\u00e3o do saldo principal. A contagem \u00e9 sempre zero quando h\u00e1 amortiza\u00e7\u00e3o do principal. Ent\u00e3o se um cliente esta h\u00e1 50 dias sem amortizar o m\u00ednimo e no dia seguinte amortiza o principal (qualquer valor), a contagem reseta. dsp bigint Contagem de dias consecutivos sem qualquer pagamento realizado. flag_transacao int Flag indicado se, na data de refer\u00eancia, houve algum valor transacionado na maquininha do cliente (TPV).","title":"portfolio_geral"},{"location":"data_dict/#portfolio_tpv","text":"Features Type Description nr_documento string N\u00famero do documento do cliente. dt_transacao string Campo que indica a data que houve algum valor transacionado na maquininha do cliente. qtd_transacoes bigint Campo que indica a quantidade de transa\u00e7\u00f5es registradas na maquininha do cliente em determinada data. vlr_tpv double Este campo se refere ao valor total transacionado na maquininha do cliente.","title":"portfolio_tpv"},{"location":"data_dict/#portfolio_comunicados","text":"Features Type Description contrato_id string Identificador \u00fanico do contrato de um cliente. dt_ref_portfolio date Data de refer\u00eancia da base de portfolio, representa a qual data um determinado registro/linha corresponde. data_acao date A data que uma a\u00e7\u00e3o de comunica\u00e7\u00e3o foi executada e enviada para o cliente. tipo_acao string Qual \u00e9 o tipo da a\u00e7\u00e3o de comunica\u00e7\u00e3o executada. acao string Tipo da a\u00e7\u00e3o de comunica\u00e7\u00e3o utilizada pelo time de cobran\u00e7a para identificar qual foi a campanha executada. Representa em qual momento da r\u00e9gua determinado contrato esta. status string Status atualizado da comunica\u00e7\u00e3o enviada, podendo ser 'Enviado', 'N\u00e3o entregue', 'Lido', 'Respondido'.","title":"portfolio_comunicados"},{"location":"data_dict/#portfolio_clientes","text":"Features Type Description nr_documento string N\u00famero do documento do cliente. tipo_empresa string Representa o tipo de empresa que corresponde do cliente MEI, CPF, CNPJ. cidade string Cidade de origem do cliente que contratou o produto de cr\u00e9dito. estado string Estado de origem (UF) do cliente que contratou o produto de cr\u00e9dito. subsegmento string Micro segmento de neg\u00f3cio em que o cliente est\u00e1 inserido. segmento string Macro segmento de neg\u00f3cio em que o cliente est\u00e1 inserido.","title":"portfolio_clientes"}]}